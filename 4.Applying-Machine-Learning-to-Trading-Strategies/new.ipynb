{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Machine Learning to Trading Strategies: Using Logistic Regression to Build Momentum-based Trading Strategies - **Patrick Beaudan and Shuoyuan He**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives :\n",
    "\n",
    "    1. Addressing the drawbacks of classical approach in building investment strategies\n",
    "    2. Use of ML Model, Logistic Regression, to build a time-series dual momentum trading strategy on the S&P 500 Index\n",
    "    3. Showing how the proposed model outperforms both buy-and-hold and several base-case dual momentum strategies, significantly increasing returns and reducing risk\n",
    "    4. Applying the algorithm to other U.S. and international large capitalization equity indices \n",
    "    5. Analyzing yields improvements in risk-adjusted performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline \n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "import yfinance as yf \n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tickers \n",
    "1. S&P 500 Index: **^GSPC**\n",
    "2. S&P Small Cap 600 Index (SML): **^SML**  ==> Data not available \n",
    "3. S&P Mid Cap 400 Index (MID): **^MID**\n",
    "4. FTSE 100 Index (UKX): **^FTSE**\n",
    "5. FTSEurofirst 300 Index (E300): **^FTEU3**  ==> Data not available\n",
    "6. Tokyo Stock Exchange Price Index (TPX): **^TPX**  ==> Data not available\n",
    "7. Dow Jones Industrial Average Index (INDU): **^DJI**\n",
    "8. Dow Jones Transportation Average Index (TRAN): **^DJT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of data :  (22844, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>2691.260010</td>\n",
       "      <td>2708.540039</td>\n",
       "      <td>2623.139893</td>\n",
       "      <td>2633.080078</td>\n",
       "      <td>2633.080078</td>\n",
       "      <td>4242240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>2630.860107</td>\n",
       "      <td>2647.510010</td>\n",
       "      <td>2583.229980</td>\n",
       "      <td>2637.719971</td>\n",
       "      <td>2637.719971</td>\n",
       "      <td>4162880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>2664.439941</td>\n",
       "      <td>2674.350098</td>\n",
       "      <td>2621.300049</td>\n",
       "      <td>2636.780029</td>\n",
       "      <td>2636.780029</td>\n",
       "      <td>3963440000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2018-12-07  2691.260010  2708.540039  2623.139893  2633.080078  2633.080078   \n",
       "2018-12-10  2630.860107  2647.510010  2583.229980  2637.719971  2637.719971   \n",
       "2018-12-11  2664.439941  2674.350098  2621.300049  2636.780029  2636.780029   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2018-12-07  4242240000  \n",
       "2018-12-10  4162880000  \n",
       "2018-12-11  3963440000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = '2018-12-12'\n",
    "\n",
    "# df_sml = yf.download('^SML',start='1993-12-31',end=end)\n",
    "df_mid = yf.download('^MID',start='1990-12-31',end=end) \n",
    "df_ukx = yf.download('^FTSE',start='1997-12-19',end=end)\n",
    "# df_e300 = yf.download('^FTEU3',start='1985-12-31',end=end)\n",
    "# df_tpx = yf.download('^TPX',start='1997-12-19',end=end)\n",
    "df_dji = yf.download('^DJI',start='1920-01-02',end=end)\n",
    "df_djt = yf.download('^DJT',start='1920-01-02',end=end) \n",
    "\n",
    "data = yf.download('^GSPC',start='1927-12-30',end=end) \n",
    "print() \n",
    "df_21 = data.copy() \n",
    "print('Shape of data : ',data.shape) \n",
    "data.tail(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining class to include base-features Momentum and Drawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Momentum features are calculated over time frames of 30, 60, 90, 120, 180, 270, 300, 360\n",
    "* Drawdown features are calculated over time frames of 15, 60, 90, 120\n",
    "\n",
    "Also, it is instructed to calculate features by skipping last month. We follow the convention of 252 business days per calendar year and 21 business days per calendar month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are selected based on the fact that observing the change in the shape of the price history using multiple historical time windows for momenta and drawdowns is more pertinent than considering other metrics to predict short-term profitability. So, we use momenta and drawdowns of different timeframes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncludeFeatures:\n",
    "    def __init__(self,data):\n",
    "        self.data = data \n",
    "\n",
    "    def calculate_momentum(self,window): # computing the rate of change in the stock's closing price over window days\n",
    "        self.data[f'momntm_{window}'] =  self.data['Adj Close'] - self.data['Adj Close'].shift(window) \n",
    "\n",
    "    def calculate_drawdown(self,window): # Compute the drawdown by finding the peak and trough in the price data\n",
    "        # calculating cumulative maximum for stocks price\n",
    "        self.data['Cumulative_Peak'] = self.data['Adj Close'].cummax() # max of cumulative value upto that day\n",
    "        # calculating drawdown \n",
    "        self.data[f'drwdwn_{window}'] = (self.data['Adj Close']-self.data['Cumulative_Peak'])/self.data['Cumulative_Peak']\n",
    "\n",
    "    def include_features(self):\n",
    "        \n",
    "        momentum_windows = [30, 60, 90, 120, 180, 270, 300, 360]\n",
    "        drawdwn_windows = [15, 60, 90, 120]    \n",
    "\n",
    "        for days in momentum_windows:\n",
    "            self.calculate_momentum(days) \n",
    "\n",
    "        for days in drawdwn_windows:\n",
    "            self.calculate_drawdown(days) \n",
    "        \n",
    "        self.data.drop(columns=['Cumulative_Peak','Open','High','Low','Close','Volume'],axis=1,inplace=True)\n",
    "        return self.data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22484, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>momntm_30</th>\n",
       "      <th>momntm_60</th>\n",
       "      <th>momntm_90</th>\n",
       "      <th>momntm_120</th>\n",
       "      <th>momntm_180</th>\n",
       "      <th>momntm_270</th>\n",
       "      <th>momntm_300</th>\n",
       "      <th>momntm_360</th>\n",
       "      <th>drwdwn_15</th>\n",
       "      <th>drwdwn_60</th>\n",
       "      <th>drwdwn_90</th>\n",
       "      <th>drwdwn_120</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-06-10</th>\n",
       "      <td>25.270000</td>\n",
       "      <td>-0.309999</td>\n",
       "      <td>-0.459999</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.090000</td>\n",
       "      <td>5.060001</td>\n",
       "      <td>6.380001</td>\n",
       "      <td>7.610001</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-06-11</th>\n",
       "      <td>25.430000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.650000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.070000</td>\n",
       "      <td>6.480000</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-06-12</th>\n",
       "      <td>25.450001</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>-0.590000</td>\n",
       "      <td>-0.289999</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.230001</td>\n",
       "      <td>5.010000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>7.730001</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close  momntm_30  momntm_60  momntm_90  momntm_120  \\\n",
       "Date                                                                 \n",
       "1929-06-10  25.270000  -0.309999  -0.459999  -0.090000        2.74   \n",
       "1929-06-11  25.430000  -0.100000  -0.650000  -0.020000        2.99   \n",
       "1929-06-12  25.450001  -0.490000  -0.590000  -0.289999        2.75   \n",
       "\n",
       "            momntm_180  momntm_270  momntm_300  momntm_360  drwdwn_15  \\\n",
       "Date                                                                    \n",
       "1929-06-10    4.090000    5.060001    6.380001    7.610001  -0.041714   \n",
       "1929-06-11    4.250000    5.070000    6.480000    7.670000  -0.035647   \n",
       "1929-06-12    4.230001    5.010000    6.170000    7.730001  -0.034888   \n",
       "\n",
       "            drwdwn_60  drwdwn_90  drwdwn_120  \n",
       "Date                                          \n",
       "1929-06-10  -0.041714  -0.041714   -0.041714  \n",
       "1929-06-11  -0.035647  -0.035647   -0.035647  \n",
       "1929-06-12  -0.034888  -0.034888   -0.034888  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_feat = IncludeFeatures(data) \n",
    "data_feat = include_feat.include_features()\n",
    "data_feat.dropna(inplace=True)\n",
    "print(data_feat.shape) \n",
    "data_feat.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values : 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Null values : {data_feat.isna().sum().sum()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyzing Key Performance Indicators over sample indices over the entire period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPIs analysed here are Annual Return, Sharpe Ratio, Volatility, Maximum Drawdown, Average Daily Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPIs:\n",
    "    def __init__(self,data):\n",
    "        self.datac = data  \n",
    "\n",
    "    def annual_return(self,datac):\n",
    "        cumulative_returns = (1+datac['Daily_Return']).prod()-1 \n",
    "        n_days = datac.shape[0]     # Number of trading days \n",
    "        annualized_return = (1+cumulative_returns)**(252/n_days)-1\n",
    "        return annualized_return \n",
    "    \n",
    "    def sharpe_ratio(self,datac):\n",
    "        average_return = datac['Daily_Return'].mean() \n",
    "        risk_free_rate = 0.01/252  # constant 1% annual risk-free rate\n",
    "        std_dev = datac['Daily_Return'].std() \n",
    "        # print(f'Average Return : {average_return:.4f}') \n",
    "        # print(f'Standard Deviation : {std_dev:.4f}') \n",
    "        # print() \n",
    "        sharpe_ratio = (average_return-risk_free_rate)/std_dev\n",
    "        return sharpe_ratio \n",
    "\n",
    "    def volatility(self,datac):\n",
    "        daily_volatility = datac['Daily_Return'].std()\n",
    "        trading_days_per_year = 252 \n",
    "        annual_volatility = daily_volatility*np.sqrt(trading_days_per_year)   # Annualizing Volatility\n",
    "        return annual_volatility \n",
    "    \n",
    "    def max_drawdown(self,datac):\n",
    "        datac['Running_max'] = datac['Adj Close'].cummax() \n",
    "        datac['Drawdowns'] = (datac['Adj Close']-datac['Running_max'])/datac['Running_max']\n",
    "\n",
    "        max_drawdown = datac['Drawdowns'].min() \n",
    "        avg_drawdown = datac['Drawdowns'].mean() \n",
    "\n",
    "        return max_drawdown, avg_drawdown \n",
    "\n",
    "    def calculate_kpi(self):        \n",
    "        self.datac['Log_Return'] =  np.log(self.datac['Adj Close']/self.datac['Adj Close'].shift(1))\n",
    "        self.datac['Daily_Return'] = self.datac['Adj Close'].pct_change() \n",
    "        self.datac.dropna(inplace=True) \n",
    "\n",
    "        annualized_return = self.annual_return(self.datac)\n",
    "        sharpe_ratio = self.sharpe_ratio(self.datac)\n",
    "        annual_volatility = self.volatility(self.datac)\n",
    "        max_drawdown, avg_drawdown = self.max_drawdown(self.datac)\n",
    "\n",
    "        print(f'Annual Return : {annualized_return*100:.1f}%')\n",
    "        print(f'Sharpe Ratio : {sharpe_ratio:.4f}')\n",
    "        print(f'Volatility : {annual_volatility*100:.0f}%')\n",
    "        print(f'Maximum Drawdown : {max_drawdown*100:.0f}%')\n",
    "        print(f'Average Drawdown : {avg_drawdown*100:.0f}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Performance Metrics of S&P 500 Index (SPX) - **^GSPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Return : 5.3%\n",
      "Sharpe Ratio : 0.0200\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -86%\n",
      "Average Drawdown : -22%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(data)   \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Performance Metrics of S&P Mid Cap 400 Index (MID) - **^MID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Return : 10.8%\n",
      "Sharpe Ratio : 0.0368\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -56%\n",
      "Average Drawdown : -7%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_mid) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Performance Metrics of FTSE 100 Index (UKX) - **^FTSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Return : 1.5%\n",
      "Sharpe Ratio : 0.0074\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -53%\n",
      "Average Drawdown : -16%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_ukx) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Performance Metrics of Dow Jones Industrial Average (INDU) - **^DJI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Return : 7.9%\n",
      "Sharpe Ratio : 0.0299\n",
      "Volatility : 17%\n",
      "Maximum Drawdown : -54%\n",
      "Average Drawdown : -9%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_dji) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Performance Metrics of Dow Jones Transportation Average (TRAN) - **^DJT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Return : 7.7%\n",
      "Sharpe Ratio : 0.0249\n",
      "Volatility : 23%\n",
      "Maximum Drawdown : -61%\n",
      "Average Drawdown : -13%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_djt)  \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Classical Time Series Dual-Momentum Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy\n",
    "\n",
    "1. The momentum, i.e. the percentage price change of a security, is calculated over a historical time horizon of twelve months, skipping the most recent month \n",
    "2. If momentum > threshold (here,5%=0.05) => Invest \n",
    "3. If momentum < threshold => the portfolio is moved to cash in the long-only strategy, or moved to a short position in the long-short strategy \n",
    "4. This investment decision is revisited at regular intervals of one month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before slicing : (22844, 6)\n",
      "Shape after slicing : (22823, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1927-12-30</th>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-03</th>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-04</th>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close  Volume\n",
       "Date                                                                     \n",
       "1927-12-30  17.660000  17.660000  17.660000  17.660000  17.660000       0\n",
       "1928-01-03  17.760000  17.760000  17.760000  17.760000  17.760000       0\n",
       "1928-01-04  17.719999  17.719999  17.719999  17.719999  17.719999       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Shape before slicing : {df_21.shape}')\n",
    "n = len(df_21)\n",
    "# Slice the DataFrame to exclude the last 21 rows for skipping most recent month \n",
    "df_21 = df_21.iloc[:n-21]\n",
    "print(f'Shape after slicing : {df_21.shape}') \n",
    "df_21.head(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating momentum, percentage change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Momentum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-01-03</th>\n",
       "      <td>24.860001</td>\n",
       "      <td>40.770107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-04</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>39.921172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-07</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>36.851021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close   Momentum\n",
       "Date                            \n",
       "1929-01-03  24.860001  40.770107\n",
       "1929-01-04  24.850000  39.921172\n",
       "1929-01-07  24.250000  36.851021"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_days_per_month = 21\n",
    "no_of_months = 12  \n",
    "time_horizon = trading_days_per_month*no_of_months \n",
    "\n",
    "df_21['Momentum'] = df_21['Adj Close'].pct_change(periods=time_horizon)*100\n",
    "df_21.dropna(inplace=True)\n",
    "df_21.drop(columns=['Open','High','Low','Close','Volume'],axis=1,inplace=True)\n",
    "\n",
    "df_21.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of invest signals :  Signals\n",
      "1    13404\n",
      "0     9167\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>Signals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-01-03</th>\n",
       "      <td>24.860001</td>\n",
       "      <td>40.770107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-04</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>39.921172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-07</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>36.851021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close   Momentum  Signals\n",
       "Date                                     \n",
       "1929-01-03  24.860001  40.770107        1\n",
       "1929-01-04  24.850000  39.921172        1\n",
       "1929-01-07  24.250000  36.851021        1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 5 \n",
    "df_21['Signals'] = (df_21['Momentum']>=threshold).astype(int) \n",
    "print('No of invest signals : ',df_21['Signals'].value_counts()) \n",
    "print() \n",
    "df_21.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Defining Function to create polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(data,degree): \n",
    "\n",
    "    feature_names = data.columns \n",
    "    # feature_names = ['Adj Close', 'momntm_30', 'momntm_60', 'momntm_90', 'momntm_120',\n",
    "    #                  'momntm_180', 'momntm_270', 'momntm_300', 'momntm_360', 'drwdwn_15',\n",
    "    #                  'drwdwn_60', 'drwdwn_90', 'drwdwn_120'] \n",
    "    \n",
    "    if data.shape[1] != len(feature_names):\n",
    "        raise ValueError(\"The number of features in the data does not match the length of feature names.\")\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    poly_feat = poly.fit_transform(data) \n",
    "    \n",
    "    feature_names_poly = poly.get_feature_names_out(input_features=feature_names)\n",
    "    \n",
    "    df_poly = pd.DataFrame(poly_feat, columns=feature_names_poly, index=data.index) \n",
    "    print(f'Shape of df_poly of degree 1 : ',data.shape) \n",
    "    print(f'Shape of df_poly of degree {degree} : ',df_poly.shape) \n",
    "    print('Number of duplicate columns : ',len(df_poly.columns)-len(set(df_poly.columns))) \n",
    "    return df_poly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_poly of degree 1 :  (22482, 17)\n",
      "Shape of df_poly of degree 2 :  (22482, 170)\n",
      "Number of duplicate columns :  0\n"
     ]
    }
   ],
   "source": [
    "x_quad = degree(data_feat,2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_poly of degree 1 :  (22482, 17)\n",
      "Shape of df_poly of degree 3 :  (22482, 1139)\n",
      "Number of duplicate columns :  0\n"
     ]
    }
   ],
   "source": [
    "x_cubic = degree(data_feat,3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating Datasets for training with Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Linear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of linear dataset before concatenation :  (22482, 17)\n",
      "Shape of linear dataset after concatenation :  (22461, 18)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of linear dataset before concatenation : ',data_feat.shape)\n",
    "x_linear = pd.concat([data_feat, df_21[['Signals']]], axis=1)\n",
    "x_linear.dropna(inplace=True) \n",
    "print('Shape of linear dataset after concatenation : ',x_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Quadratic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of quadratic dataset before concatenation :  (22482, 170)\n",
      "Shape of quadratic dataset after concatenation :  (22461, 171)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of quadratic dataset before concatenation : ',x_quad.shape)\n",
    "x_quad = pd.concat([x_quad, df_21[['Signals']]], axis=1)\n",
    "x_quad.dropna(inplace=True) \n",
    "print('Shape of quadratic dataset after concatenation : ',x_quad.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Cubic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cubic dataset before concatenation :  (22482, 1139)\n",
      "Shape of cubic dataset after concatenation :  (22461, 1140)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of cubic dataset before concatenation : ',x_cubic.shape)\n",
    "x_cubic = pd.concat([x_cubic, df_21[['Signals']]], axis=1)\n",
    "x_cubic.dropna(inplace=True) \n",
    "print('Shape of cubic dataset after concatenation : ',x_cubic.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Class for Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model metrics calculated are cost function, accuracy, confusion matrix and classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the cost function, also known as the loss function, for logistic regression, we need to use the logistic loss function, which is commonly referred to as cross-entropy loss or log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.train_size = 0.4\n",
    "        self.random_state = 42\n",
    "\n",
    "    def scaling_x(self,X):\n",
    "        scaler = StandardScaler()\n",
    "        scaled_X = scaler.fit_transform(X)\n",
    "        return scaled_X\n",
    "    \n",
    "    def cost_func(self,model,x_test,y_test): \n",
    "        probabilities = model.predict_proba(x_test)[:,1] # Getting probabilities for class 1 (positive class)\n",
    "        cost = log_loss(y_test,probabilities) \n",
    "        return cost \n",
    "\n",
    "    def model_metrics(self,model,x_test,y_test):\n",
    "        y_pred = model.predict(x_test) \n",
    "        cost_fn = self.cost_func(model,x_test,y_test)\n",
    "        accuracy = accuracy_score(y_test,y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'Cost function : {cost_fn}') \n",
    "        print(f'Accuracy : {accuracy}')\n",
    "        print('Confusion Matrix : ')\n",
    "        print(conf_matrix) \n",
    "        print('Classification Report : ')\n",
    "        print(class_report) \n",
    "        return y_pred \n",
    "    \n",
    "    def training_model(self,df):\n",
    "\n",
    "        X = df.drop(columns=['Signals'],axis=1)\n",
    "        Y = df['Signals'] \n",
    "\n",
    "        scaled_X = self.scaling_x(X)\n",
    "        # Split the data into initial training set (40%) and test set (60%)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(scaled_X,Y,train_size=0.4, shuffle=False, \n",
    "                                                            random_state=42)\n",
    "        model = LogisticRegression(C=1.0)   # C is the regularization parameter\n",
    "        \n",
    "        model.fit(x_train,y_train) \n",
    "            \n",
    "        self.model_metrics(model,x_test,y_test) \n",
    "\n",
    "logistic = logistic_regression()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation of Linear, Quadratic and Cubic Combination of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Evaluation on Linear Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function : 0.9787919508670793\n",
      "Accuracy : 0.9158566446538547\n",
      "Confusion Matrix : \n",
      "[[4067  854]\n",
      " [ 280 8276]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.83      0.88      4921\n",
      "         1.0       0.91      0.97      0.94      8556\n",
      "\n",
      "    accuracy                           0.92     13477\n",
      "   macro avg       0.92      0.90      0.91     13477\n",
      "weighted avg       0.92      0.92      0.91     13477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_linear = logistic.training_model(x_linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Evaluation on Quadratic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function : 1.0260370337067828\n",
      "Accuracy : 0.9174148549380426\n",
      "Confusion Matrix : \n",
      "[[4088  833]\n",
      " [ 280 8276]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.83      0.88      4921\n",
      "         1.0       0.91      0.97      0.94      8556\n",
      "\n",
      "    accuracy                           0.92     13477\n",
      "   macro avg       0.92      0.90      0.91     13477\n",
      "weighted avg       0.92      0.92      0.92     13477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_quad = logistic.training_model(x_quad)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Evaluation on Cubic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function : 1.0535235752512844\n",
      "Accuracy : 0.9129628255546487\n",
      "Confusion Matrix : \n",
      "[[3985  936]\n",
      " [ 237 8319]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.81      0.87      4921\n",
      "         1.0       0.90      0.97      0.93      8556\n",
      "\n",
      "    accuracy                           0.91     13477\n",
      "   macro avg       0.92      0.89      0.90     13477\n",
      "weighted avg       0.92      0.91      0.91     13477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_cubic = logistic.training_model(x_cubic)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Sliding Window over Cubic Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training on an initial set of data (40% of data)\n",
    "2. Training calibrate parameters before applying to testing sets\n",
    "3. Convergence is monitored by cost function. \n",
    "4. Convergence is achieved when (cost function < threshold)  => Threshold=0.01 \n",
    "5. After convergence, training set is slid by a window of 5 or 10 years to include more recent data and model is retrained. This helps the model to train over current market conditions \n",
    "6. Retraining is done every 50 days \n",
    "7. Continue this till the end of data\n",
    "\n",
    "This approach ensures that your model remains updated with recent data and can adapt to changing market conditions effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    def __init__(self,data):\n",
    "        self.data = data \n",
    "        self.conv_interval = 50  # days\n",
    "        self.retrain_freq = 252*8   # 8 years = 8*252 days\n",
    "        self.tolerance = 0.0001\n",
    "\n",
    "    def scaling_x(self, X):\n",
    "        scaler = StandardScaler()\n",
    "        scaled_x = scaler.fit_transform(X)\n",
    "        return scaled_x \n",
    "\n",
    "    def cost_funcn(self, model, X, Y):\n",
    "        y_prob = model.predict_proba(X)[:,1]     # Getting probabilities for class 1 (positive class)\n",
    "        cost = log_loss(Y, y_prob) \n",
    "        return cost     \n",
    "    \n",
    "    def data_concat(self, y_pred, start, end): \n",
    "        datac = self.data[start:end] \n",
    "        y_pred = pd.Series(y_pred,index=datac.index) \n",
    "\n",
    "        bnch_df = datac[datac['Signals'] == 1].copy() \n",
    "        print('='*20,'Metric for bnch_df','='*20) \n",
    "        calc_kpi = KPIs(bnch_df)  \n",
    "        calc_kpi.calculate_kpi() \n",
    "\n",
    "        log_indices = y_pred[y_pred==1].index \n",
    "        log_df = self.data.loc[log_indices].copy()  # Filter original DataFrame based on indices\n",
    "        log_df['y_pred'] = y_pred[log_indices]     # Add y_pred column \n",
    "        log_df = log_df[['y_pred', 'Signals', 'Adj Close']] \n",
    "        # print(log_df.head(3))    \n",
    "        print('='*20,'Metric for log_df','='*20) \n",
    "        calc_kpi = KPIs(log_df)  \n",
    "        calc_kpi.calculate_kpi()\n",
    "        \n",
    "    def model_metrics(self, model, X, Y, start, end):\n",
    "        y_pred = model.predict(X) \n",
    "        cost_fn = self.cost_funcn(model, X, Y)\n",
    "        accuracy = accuracy_score(Y, y_pred)\n",
    "        conf_matrix = confusion_matrix(Y, y_pred)\n",
    "        class_report = classification_report(Y, y_pred)  \n",
    "        \n",
    "        print(f'Cost Function : {cost_fn}')\n",
    "        print(f'Accuracy Score : {accuracy}')\n",
    "        print('Confusion Matrix :') \n",
    "        print(conf_matrix) \n",
    "        print('Classification Report : ')\n",
    "        print(class_report) \n",
    "        \n",
    "        self.data_concat(y_pred, start, end)\n",
    "    \n",
    "    def date_correction(self,indx,df,num):\n",
    "        idx1 = df.index.get_loc(indx)\n",
    "        idx2 = idx1 + num \n",
    "        if idx2<len(df)-1:\n",
    "            return idx2 \n",
    "        else:\n",
    "            return len(df)-1   \n",
    "    \n",
    "    def training_logistic(self, df):\n",
    "\n",
    "        # Initialize Parameters        \n",
    "        train_start = df.index[0]\n",
    "        train_end = df.index[int(0.4*len(df))] \n",
    "        test_start = train_end \n",
    "        idx = df.index.get_loc(test_start)+252*8 \n",
    "        test_end = df.index[idx] \n",
    "\n",
    "        model = LogisticRegression() \n",
    "\n",
    "        # Initial Training set\n",
    "        x_train = df.loc[train_start:train_end].drop('Signals', axis=1) \n",
    "        xs_train = self.scaling_x(x_train) \n",
    "        y_train = df.loc[train_start:train_end, 'Signals'] \n",
    "\n",
    "        x_test = df.loc[test_start:test_end].drop('Signals', axis=1) \n",
    "        xs_test = self.scaling_x(x_test) \n",
    "        y_test = df.loc[test_start:test_end, 'Signals'] \n",
    "\n",
    "        model.fit(xs_train, y_train)\n",
    "\n",
    "        print(f'Train set interval: {str(train_start).split(' 00:00:00')[0]} to {str(train_end).split(' 00:00:00')[0]}')       \n",
    "        print() \n",
    "        print('='*20,'Metrics','='*20) \n",
    "        self.model_metrics(model, xs_train, y_train, train_start, train_end)\n",
    "\n",
    "        print(f'Test set interval: {str(test_start).split(' 00:00:00')[0]} to {str(test_end).split(' 00:00:00')[0]}')       \n",
    "        print()\n",
    "        print('='*20,'Metrics','='*20) \n",
    "        self.model_metrics(model, xs_test, y_test, test_start, test_end)\n",
    "         \n",
    "\n",
    "        # Training Loop  \n",
    "        while test_end<df.index[-1]:\n",
    "\n",
    "            model.fit(xs_train, y_train)\n",
    "\n",
    "            # Loop for checking convergence\n",
    "            previous_cost = None \n",
    "\n",
    "            while test_end<df.index[-1]:\n",
    "                x_test = df.loc[test_start:test_end].drop('Signals', axis=1) \n",
    "                xs_test = self.scaling_x(x_test) \n",
    "                y_test = df.loc[test_start:test_end, 'Signals'] \n",
    "                            \n",
    "                current_cost = self.cost_funcn(model,xs_test,y_test) \n",
    "\n",
    "                if previous_cost is not None and (previous_cost-current_cost)/previous_cost < self.tolerance:\n",
    "                    print() \n",
    "                    print(f'Convergence achieved at {str(test_end).split(' 00:00:00')[0]}') \n",
    "                    break   \n",
    "                \n",
    "                previous_cost = current_cost\n",
    "                idx = self.date_correction(test_end,df,self.conv_interval)  \n",
    "                test_end = df.index[idx] \n",
    "\n",
    "            # Slide the training window \n",
    "            idxt1 = self.date_correction(train_start, df,self.retrain_freq)\n",
    "            train_start = df.index[idxt1] \n",
    "\n",
    "            idxt2 = self.date_correction(train_end, df,self.retrain_freq)\n",
    "            train_end = df.index[idxt2]\n",
    "\n",
    "            test_start = train_end \n",
    "\n",
    "            idxs = self.date_correction(test_end, df,self.retrain_freq)\n",
    "            test_end = df.index[idxs]  \n",
    "\n",
    "            # Updating training data\n",
    "            \n",
    "            if train_end<=df.index[-1]:\n",
    "                x_train = df.loc[train_start:train_end].drop('Signals', axis=1) \n",
    "                xs_train = self.scaling_x(x_train) \n",
    "                y_train = df.loc[train_start:train_end, 'Signals'] \n",
    "\n",
    "            print(f'Train set interval: {str(train_start).split(' 00:00:00')[0]} to {str(train_end).split(' 00:00:00')[0]}')       \n",
    "            print()\n",
    "            print('='*20,'Metrics','='*20) \n",
    "            model_m = model.fit(xs_train,y_train) \n",
    "            self.model_metrics(model_m, xs_train, y_train, train_start, train_end)   \n",
    "\n",
    "            # Updating Testing data\n",
    "            \n",
    "            if test_end<=df.index[-1]:\n",
    "                x_test = df.loc[test_start:test_end].drop('Signals', axis=1) \n",
    "                xs_test = self.scaling_x(x_test) \n",
    "                y_test = df.loc[test_start:test_end, 'Signals'] \n",
    "\n",
    "            print(f'Test set interval: {str(test_start).split(' 00:00:00')[0]} to {str(test_end).split(' 00:00:00')[0]}')       \n",
    "            print()\n",
    "            print('='*20,'Metrics','='*20) \n",
    "            self.model_metrics(model_m, xs_test, y_test, test_start, test_end)   \n",
    "            print('*'*100)\n",
    "strategy = Strategy(x_linear)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set interval: 1929-06-12 to 1965-04-29\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.1125953039703724\n",
      "Accuracy Score : 0.9549248747913188\n",
      "Confusion Matrix :\n",
      "[[4054  192]\n",
      " [ 213 4526]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.95      0.95      4246\n",
      "         1.0       0.96      0.96      0.96      4739\n",
      "\n",
      "    accuracy                           0.95      8985\n",
      "   macro avg       0.95      0.95      0.95      8985\n",
      "weighted avg       0.95      0.95      0.95      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 6.9%\n",
      "Sharpe Ratio : 0.0260\n",
      "Volatility : 25%\n",
      "Maximum Drawdown : -78%\n",
      "Average Drawdown : -31%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 6.9%\n",
      "Sharpe Ratio : 0.0256\n",
      "Volatility : 24%\n",
      "Maximum Drawdown : -77%\n",
      "Average Drawdown : -30%\n",
      "Test set interval: 1965-04-29 to 1973-06-07\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.4127768147100789\n",
      "Accuracy Score : 0.8820029747149232\n",
      "Confusion Matrix :\n",
      "[[ 704  206]\n",
      " [  32 1075]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.77      0.86       910\n",
      "         1.0       0.84      0.97      0.90      1107\n",
      "\n",
      "    accuracy                           0.88      2017\n",
      "   macro avg       0.90      0.87      0.88      2017\n",
      "weighted avg       0.89      0.88      0.88      2017\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 5.5%\n",
      "Sharpe Ratio : 0.0287\n",
      "Volatility : 11%\n",
      "Maximum Drawdown : -17%\n",
      "Average Drawdown : -4%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 4.9%\n",
      "Sharpe Ratio : 0.0276\n",
      "Volatility : 10%\n",
      "Maximum Drawdown : -15%\n",
      "Average Drawdown : -4%\n",
      "\n",
      "Convergence achieved at 1973-08-17\n",
      "Train set interval: 1937-07-13 to 1973-06-07\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.15768211046357103\n",
      "Accuracy Score : 0.9397885364496383\n",
      "Confusion Matrix :\n",
      "[[3761  252]\n",
      " [ 289 4683]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.94      0.93      4013\n",
      "         1.0       0.95      0.94      0.95      4972\n",
      "\n",
      "    accuracy                           0.94      8985\n",
      "   macro avg       0.94      0.94      0.94      8985\n",
      "weighted avg       0.94      0.94      0.94      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 10.3%\n",
      "Sharpe Ratio : 0.0425\n",
      "Volatility : 15%\n",
      "Maximum Drawdown : -46%\n",
      "Average Drawdown : -7%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 10.3%\n",
      "Sharpe Ratio : 0.0451\n",
      "Volatility : 14%\n",
      "Maximum Drawdown : -45%\n",
      "Average Drawdown : -7%\n",
      "Test set interval: 1973-06-07 to 1981-08-11\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.3499402519052555\n",
      "Accuracy Score : 0.8896952104499274\n",
      "Confusion Matrix :\n",
      "[[859 211]\n",
      " [ 17 980]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.80      0.88      1070\n",
      "         1.0       0.82      0.98      0.90       997\n",
      "\n",
      "    accuracy                           0.89      2067\n",
      "   macro avg       0.90      0.89      0.89      2067\n",
      "weighted avg       0.90      0.89      0.89      2067\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 9.8%\n",
      "Sharpe Ratio : 0.0420\n",
      "Volatility : 14%\n",
      "Maximum Drawdown : -14%\n",
      "Average Drawdown : -4%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 9.3%\n",
      "Sharpe Ratio : 0.0424\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -14%\n",
      "Average Drawdown : -5%\n",
      "****************************************************************************************************\n",
      "\n",
      "Convergence achieved at 1981-10-21\n",
      "Train set interval: 1945-07-31 to 1981-06-01\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.15913372237627768\n",
      "Accuracy Score : 0.9439065108514191\n",
      "Confusion Matrix :\n",
      "[[3659  243]\n",
      " [ 261 4822]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.94      0.94      3902\n",
      "         1.0       0.95      0.95      0.95      5083\n",
      "\n",
      "    accuracy                           0.94      8985\n",
      "   macro avg       0.94      0.94      0.94      8985\n",
      "weighted avg       0.94      0.94      0.94      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 11.5%\n",
      "Sharpe Ratio : 0.0522\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 11.6%\n",
      "Sharpe Ratio : 0.0541\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "Test set interval: 1981-06-01 to 1989-10-11\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.3815596831856732\n",
      "Accuracy Score : 0.9258384506376949\n",
      "Confusion Matrix :\n",
      "[[ 757    0]\n",
      " [ 157 1203]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      1.00      0.91       757\n",
      "         1.0       1.00      0.88      0.94      1360\n",
      "\n",
      "    accuracy                           0.93      2117\n",
      "   macro avg       0.91      0.94      0.92      2117\n",
      "weighted avg       0.94      0.93      0.93      2117\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 20.2%\n",
      "Sharpe Ratio : 0.0763\n",
      "Volatility : 15%\n",
      "Maximum Drawdown : -23%\n",
      "Average Drawdown : -4%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 23.1%\n",
      "Sharpe Ratio : 0.0872\n",
      "Volatility : 15%\n",
      "Maximum Drawdown : -18%\n",
      "Average Drawdown : -3%\n",
      "****************************************************************************************************\n",
      "\n",
      "Convergence achieved at 1989-12-21\n",
      "Train set interval: 1953-08-27 to 1989-05-19\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.15426831919857228\n",
      "Accuracy Score : 0.9474680022259321\n",
      "Confusion Matrix :\n",
      "[[3591  269]\n",
      " [ 203 4922]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94      3860\n",
      "         1.0       0.95      0.96      0.95      5125\n",
      "\n",
      "    accuracy                           0.95      8985\n",
      "   macro avg       0.95      0.95      0.95      8985\n",
      "weighted avg       0.95      0.95      0.95      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 13.0%\n",
      "Sharpe Ratio : 0.0572\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 12.5%\n",
      "Sharpe Ratio : 0.0569\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "Test set interval: 1989-05-19 to 1997-12-11\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 1.209869599467391\n",
      "Accuracy Score : 0.6289801568989386\n",
      "Confusion Matrix :\n",
      "[[447   0]\n",
      " [804 916]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      1.00      0.53       447\n",
      "         1.0       1.00      0.53      0.69      1720\n",
      "\n",
      "    accuracy                           0.63      2167\n",
      "   macro avg       0.68      0.77      0.61      2167\n",
      "weighted avg       0.87      0.63      0.66      2167\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 17.3%\n",
      "Sharpe Ratio : 0.0785\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -11%\n",
      "Average Drawdown : -2%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 35.0%\n",
      "Sharpe Ratio : 0.1188\n",
      "Volatility : 16%\n",
      "Maximum Drawdown : -11%\n",
      "Average Drawdown : -2%\n",
      "****************************************************************************************************\n",
      "\n",
      "Convergence achieved at 1998-02-25\n",
      "Train set interval: 1961-08-31 to 1997-05-09\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.17637781953284784\n",
      "Accuracy Score : 0.946800222593211\n",
      "Confusion Matrix :\n",
      "[[3147  301]\n",
      " [ 177 5360]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93      3448\n",
      "         1.0       0.95      0.97      0.96      5537\n",
      "\n",
      "    accuracy                           0.95      8985\n",
      "   macro avg       0.95      0.94      0.94      8985\n",
      "weighted avg       0.95      0.95      0.95      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 12.0%\n",
      "Sharpe Ratio : 0.0537\n",
      "Volatility : 13%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 11.7%\n",
      "Sharpe Ratio : 0.0555\n",
      "Volatility : 12%\n",
      "Maximum Drawdown : -32%\n",
      "Average Drawdown : -5%\n",
      "Test set interval: 1997-05-09 to 2006-03-02\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.2530721604214895\n",
      "Accuracy Score : 0.9088858818222824\n",
      "Confusion Matrix :\n",
      "[[ 645  122]\n",
      " [  80 1370]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.84      0.86       767\n",
      "         1.0       0.92      0.94      0.93      1450\n",
      "\n",
      "    accuracy                           0.91      2217\n",
      "   macro avg       0.90      0.89      0.90      2217\n",
      "weighted avg       0.91      0.91      0.91      2217\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 8.1%\n",
      "Sharpe Ratio : 0.0277\n",
      "Volatility : 21%\n",
      "Maximum Drawdown : -37%\n",
      "Average Drawdown : -12%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 7.8%\n",
      "Sharpe Ratio : 0.0269\n",
      "Volatility : 21%\n",
      "Maximum Drawdown : -39%\n",
      "Average Drawdown : -14%\n",
      "****************************************************************************************************\n",
      "\n",
      "Convergence achieved at 2006-07-25\n",
      "Train set interval: 1969-10-15 to 2005-05-16\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.194522271281583\n",
      "Accuracy Score : 0.9479131886477462\n",
      "Confusion Matrix :\n",
      "[[3131  280]\n",
      " [ 188 5386]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.92      0.93      3411\n",
      "         1.0       0.95      0.97      0.96      5574\n",
      "\n",
      "    accuracy                           0.95      8985\n",
      "   macro avg       0.95      0.94      0.94      8985\n",
      "weighted avg       0.95      0.95      0.95      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 12.0%\n",
      "Sharpe Ratio : 0.0459\n",
      "Volatility : 16%\n",
      "Maximum Drawdown : -37%\n",
      "Average Drawdown : -7%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 12.0%\n",
      "Sharpe Ratio : 0.0473\n",
      "Volatility : 15%\n",
      "Maximum Drawdown : -37%\n",
      "Average Drawdown : -6%\n",
      "Test set interval: 2005-05-16 to 2014-07-29\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.16402321941939058\n",
      "Accuracy Score : 0.9257660768234787\n",
      "Confusion Matrix :\n",
      "[[ 593  137]\n",
      " [  35 1552]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.81      0.87       730\n",
      "         1.0       0.92      0.98      0.95      1587\n",
      "\n",
      "    accuracy                           0.93      2317\n",
      "   macro avg       0.93      0.90      0.91      2317\n",
      "weighted avg       0.93      0.93      0.92      2317\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 8.7%\n",
      "Sharpe Ratio : 0.0314\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -35%\n",
      "Average Drawdown : -9%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 8.1%\n",
      "Sharpe Ratio : 0.0297\n",
      "Volatility : 18%\n",
      "Maximum Drawdown : -35%\n",
      "Average Drawdown : -9%\n",
      "****************************************************************************************************\n",
      "\n",
      "Convergence achieved at 2015-07-27\n",
      "Train set interval: 1977-10-07 to 2013-05-20\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.19492935403870493\n",
      "Accuracy Score : 0.9246521981079577\n",
      "Confusion Matrix :\n",
      "[[2642  407]\n",
      " [ 270 5666]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.87      0.89      3049\n",
      "         1.0       0.93      0.95      0.94      5936\n",
      "\n",
      "    accuracy                           0.92      8985\n",
      "   macro avg       0.92      0.91      0.92      8985\n",
      "weighted avg       0.92      0.92      0.92      8985\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 12.5%\n",
      "Sharpe Ratio : 0.0451\n",
      "Volatility : 17%\n",
      "Maximum Drawdown : -37%\n",
      "Average Drawdown : -8%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 12.2%\n",
      "Sharpe Ratio : 0.0460\n",
      "Volatility : 16%\n",
      "Maximum Drawdown : -37%\n",
      "Average Drawdown : -7%\n",
      "Test set interval: 2013-05-20 to 2018-11-08\n",
      "\n",
      "==================== Metrics ====================\n",
      "Cost Function : 0.8831694647550089\n",
      "Accuracy Score : 0.8196958725561188\n",
      "Confusion Matrix :\n",
      "[[262   5]\n",
      " [244 870]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.98      0.68       267\n",
      "         1.0       0.99      0.78      0.87      1114\n",
      "\n",
      "    accuracy                           0.82      1381\n",
      "   macro avg       0.76      0.88      0.78      1381\n",
      "weighted avg       0.90      0.82      0.84      1381\n",
      "\n",
      "==================== Metric for bnch_df ====================\n",
      "Annual Return : 12.5%\n",
      "Sharpe Ratio : 0.0626\n",
      "Volatility : 12%\n",
      "Maximum Drawdown : -10%\n",
      "Average Drawdown : -2%\n",
      "==================== Metric for log_df ====================\n",
      "Annual Return : 17.1%\n",
      "Sharpe Ratio : 0.0918\n",
      "Volatility : 11%\n",
      "Maximum Drawdown : -10%\n",
      "Average Drawdown : -1%\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "strategy.training_logistic(x_linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calculating Key Performance Indicators of various Logistic regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 Benchmark SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2 Logistic Regression Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3 Logistic Regression Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.4 Logistic Regression Cubic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Retraining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "Frequency at which training set should be revised on regular intervals as new data is generated in market\n",
    "1. Retraining period is about 5 and 10 years for one asset \n",
    "2. But for a portfolio with multiple assets, this approach is not feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
