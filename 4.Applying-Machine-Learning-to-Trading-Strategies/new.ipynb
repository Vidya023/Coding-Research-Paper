{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Machine Learning to Trading Strategies: Using Logistic Regression to Build Momentum-based Trading Strategies - **Patrick Beaudan and Shuoyuan He**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives :\n",
    "\n",
    "    1. Addressing the drawbacks of classical approach in building investment strategies\n",
    "    2. Use of ML Model, Logistic Regression, to build a time-series dual momentum trading strategy on the S&P 500 Index\n",
    "    3. Showing how the proposed model outperforms both buy-and-hold and several base-case dual momentum strategies, significantly increasing returns and reducing risk\n",
    "    4. Applying the algorithm to other U.S. and international large capitalization equity indices \n",
    "    5. Analyzing yields improvements in risk-adjusted performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline \n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "import yfinance as yf \n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tickers \n",
    "1. S&P 500 Index: **^GSPC**\n",
    "2. S&P Small Cap 600 Index (SML): **^SML**  ==> Data not available \n",
    "3. S&P Mid Cap 400 Index (MID): **^MID**\n",
    "4. FTSE 100 Index (UKX): **^FTSE**\n",
    "5. FTSEurofirst 300 Index (E300): **^FTEU3**  ==> Data not available\n",
    "6. Tokyo Stock Exchange Price Index (TPX): **^TPX**  ==> Data not available\n",
    "7. Dow Jones Industrial Average Index (INDU): **^DJI**\n",
    "8. Dow Jones Transportation Average Index (TRAN): **^DJT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of data :  (22844, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>2691.260010</td>\n",
       "      <td>2708.540039</td>\n",
       "      <td>2623.139893</td>\n",
       "      <td>2633.080078</td>\n",
       "      <td>2633.080078</td>\n",
       "      <td>4242240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>2630.860107</td>\n",
       "      <td>2647.510010</td>\n",
       "      <td>2583.229980</td>\n",
       "      <td>2637.719971</td>\n",
       "      <td>2637.719971</td>\n",
       "      <td>4162880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>2664.439941</td>\n",
       "      <td>2674.350098</td>\n",
       "      <td>2621.300049</td>\n",
       "      <td>2636.780029</td>\n",
       "      <td>2636.780029</td>\n",
       "      <td>3963440000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2018-12-07  2691.260010  2708.540039  2623.139893  2633.080078  2633.080078   \n",
       "2018-12-10  2630.860107  2647.510010  2583.229980  2637.719971  2637.719971   \n",
       "2018-12-11  2664.439941  2674.350098  2621.300049  2636.780029  2636.780029   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2018-12-07  4242240000  \n",
       "2018-12-10  4162880000  \n",
       "2018-12-11  3963440000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = '2018-12-12'\n",
    "\n",
    "# df_sml = yf.download('^SML',start='1993-12-31',end=end)\n",
    "df_mid = yf.download('^MID',start='1990-12-31',end=end) \n",
    "df_ukx = yf.download('^FTSE',start='1997-12-19',end=end)\n",
    "# df_e300 = yf.download('^FTEU3',start='1985-12-31',end=end)\n",
    "# df_tpx = yf.download('^TPX',start='1997-12-19',end=end)\n",
    "df_dji = yf.download('^DJI',start='1920-01-02',end=end)\n",
    "df_djt = yf.download('^DJT',start='1920-01-02',end=end) \n",
    "\n",
    "data = yf.download('^GSPC',start='1927-12-30',end=end) \n",
    "print() \n",
    "df_21 = data.copy() \n",
    "print('Shape of data : ',data.shape) \n",
    "data.tail(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining class to include base-features Momentum and Drawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Momentum features are calculated over time frames of 30, 60, 90, 120, 180, 270, 300, 360\n",
    "* Drawdown features are calculated over time frames of 15, 60, 90, 120\n",
    "\n",
    "Also, it is instructed to calculate features by skipping last month. We follow the convention of 252 business days per calendar year and 21 business days per calendar month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are selected based on the fact that observing the change in the shape of the price history using multiple historical time windows for momenta and drawdowns is more pertinent than considering other metrics to predict short-term profitability. So, we use momenta and drawdowns of different timeframes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncludeFeatures:\n",
    "    def __init__(self,data):\n",
    "        self.data = data \n",
    "\n",
    "    def calculate_momentum(self,window): # computing the rate of change in the stock's closing price over window days\n",
    "        self.data[f'momntm_{window}'] =  self.data['Adj Close'] - self.data['Adj Close'].shift(window) \n",
    "\n",
    "    def calculate_drawdown(self,window): # Compute the drawdown by finding the peak and trough in the price data\n",
    "        # calculating cumulative maximum for stocks price\n",
    "        self.data['Cumulative_Peak'] = self.data['Adj Close'].cummax() # max of cumulative value upto that day\n",
    "        # calculating drawdown \n",
    "        self.data[f'drwdwn_{window}'] = (self.data['Adj Close']-self.data['Cumulative_Peak'])/self.data['Cumulative_Peak']\n",
    "\n",
    "    def include_features(self):\n",
    "        \n",
    "        momentum_windows = [30, 60, 90, 120, 180, 270, 300, 360]\n",
    "        drawdwn_windows = [15, 60, 90, 120]    \n",
    "\n",
    "        for days in momentum_windows:\n",
    "            self.calculate_momentum(days) \n",
    "\n",
    "        for days in drawdwn_windows:\n",
    "            self.calculate_drawdown(days) \n",
    "        \n",
    "        self.data.drop(columns=['Cumulative_Peak','Open','High','Low','Close','Volume'],axis=1,inplace=True)\n",
    "        return self.data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22484, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>momntm_30</th>\n",
       "      <th>momntm_60</th>\n",
       "      <th>momntm_90</th>\n",
       "      <th>momntm_120</th>\n",
       "      <th>momntm_180</th>\n",
       "      <th>momntm_270</th>\n",
       "      <th>momntm_300</th>\n",
       "      <th>momntm_360</th>\n",
       "      <th>drwdwn_15</th>\n",
       "      <th>drwdwn_60</th>\n",
       "      <th>drwdwn_90</th>\n",
       "      <th>drwdwn_120</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-06-10</th>\n",
       "      <td>25.270000</td>\n",
       "      <td>-0.309999</td>\n",
       "      <td>-0.459999</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.090000</td>\n",
       "      <td>5.060001</td>\n",
       "      <td>6.380001</td>\n",
       "      <td>7.610001</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "      <td>-0.041714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-06-11</th>\n",
       "      <td>25.430000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.650000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.070000</td>\n",
       "      <td>6.480000</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>-0.035647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-06-12</th>\n",
       "      <td>25.450001</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>-0.590000</td>\n",
       "      <td>-0.289999</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.230001</td>\n",
       "      <td>5.010000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>7.730001</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "      <td>-0.034888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close  momntm_30  momntm_60  momntm_90  momntm_120  \\\n",
       "Date                                                                 \n",
       "1929-06-10  25.270000  -0.309999  -0.459999  -0.090000        2.74   \n",
       "1929-06-11  25.430000  -0.100000  -0.650000  -0.020000        2.99   \n",
       "1929-06-12  25.450001  -0.490000  -0.590000  -0.289999        2.75   \n",
       "\n",
       "            momntm_180  momntm_270  momntm_300  momntm_360  drwdwn_15  \\\n",
       "Date                                                                    \n",
       "1929-06-10    4.090000    5.060001    6.380001    7.610001  -0.041714   \n",
       "1929-06-11    4.250000    5.070000    6.480000    7.670000  -0.035647   \n",
       "1929-06-12    4.230001    5.010000    6.170000    7.730001  -0.034888   \n",
       "\n",
       "            drwdwn_60  drwdwn_90  drwdwn_120  \n",
       "Date                                          \n",
       "1929-06-10  -0.041714  -0.041714   -0.041714  \n",
       "1929-06-11  -0.035647  -0.035647   -0.035647  \n",
       "1929-06-12  -0.034888  -0.034888   -0.034888  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_feat = IncludeFeatures(data) \n",
    "data_feat = include_feat.include_features()\n",
    "data_feat.dropna(inplace=True)\n",
    "print(data_feat.shape) \n",
    "data_feat.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values : 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Null values : {data_feat.isna().sum().sum()}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyzing Key Performance Indicators over sample indices over the entire period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPIs analysed here are Annual Return, Sharpe Ratio, Volatility, Maximum Drawdown, Average Daily Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPIs:\n",
    "    def __init__(self,data):\n",
    "        self.datac = data  \n",
    "\n",
    "    def annual_return(self,datac):\n",
    "        cumulative_returns = (1+datac['Daily_Return']).prod()-1 \n",
    "        n_days = datac.shape[0]     # Number of trading days \n",
    "        annualized_return = (1+cumulative_returns)**(252/n_days)-1\n",
    "        return annualized_return \n",
    "    \n",
    "    def sharpe_ratio(self,datac):\n",
    "        average_return = datac['Daily_Return'].mean() \n",
    "        risk_free_rate = 0.01/252  # constant 1% annual risk-free rate\n",
    "        std_dev = datac['Daily_Return'].std() \n",
    "        print(f'Average Return : {average_return:.4f}') \n",
    "        print(f'Standard Deviation : {std_dev:.4f}') \n",
    "        print() \n",
    "        sharpe_ratio = (average_return-risk_free_rate)/std_dev\n",
    "        return sharpe_ratio \n",
    "\n",
    "    def volatility(self,datac):\n",
    "        daily_volatility = datac['Daily_Return'].std()\n",
    "        trading_days_per_year = 252 \n",
    "        annual_volatility = daily_volatility*np.sqrt(trading_days_per_year)   # Annualizing Volatility\n",
    "        return annual_volatility \n",
    "    \n",
    "    def max_drawdown(self,datac):\n",
    "        datac['Running_max'] = datac['Adj Close'].cummax() \n",
    "        datac['Drawdowns'] = (datac['Adj Close']-datac['Running_max'])/datac['Running_max']\n",
    "\n",
    "        max_drawdown = datac['Drawdowns'].min() \n",
    "        avg_drawdown = datac['Drawdowns'].mean() \n",
    "\n",
    "        return max_drawdown, avg_drawdown \n",
    "\n",
    "    def calculate_kpi(self):        \n",
    "        self.datac['Log_Return'] =  np.log(self.datac['Adj Close']/self.datac['Adj Close'].shift(1))\n",
    "        self.datac['Daily_Return'] = self.datac['Adj Close'].pct_change() \n",
    "        self.datac.dropna(inplace=True) \n",
    "\n",
    "        annualized_return = self.annual_return(self.datac)\n",
    "        sharpe_ratio = self.sharpe_ratio(self.datac)\n",
    "        annual_volatility = self.volatility(self.datac)\n",
    "        max_drawdown, avg_drawdown = self.max_drawdown(self.datac)\n",
    "\n",
    "        print(f'Annual Return : {annualized_return*100:.1f}%')\n",
    "        print(f'Sharpe Ratio : {sharpe_ratio:.4f}')\n",
    "        print(f'Volatility : {annual_volatility*100:.0f}%')\n",
    "        print(f'Maximum Drawdown : {max_drawdown*100:.0f}%')\n",
    "        print(f'Average Drawdown : {avg_drawdown*100:.0f}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Performance Metrics of S&P 500 Index (SPX) - **^GSPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return : 0.0003\n",
      "Standard Deviation : 0.0119\n",
      "\n",
      "Annual Return : 5.3%\n",
      "Sharpe Ratio : 0.0200\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -86%\n",
      "Average Drawdown : -22%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(data)   \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Performance Metrics of S&P Mid Cap 400 Index (MID) - **^MID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return : 0.0005\n",
      "Standard Deviation : 0.0120\n",
      "\n",
      "Annual Return : 10.8%\n",
      "Sharpe Ratio : 0.0367\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -56%\n",
      "Average Drawdown : -7%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_mid) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Performance Metrics of FTSE 100 Index (UKX) - **^FTSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return : 0.0001\n",
      "Standard Deviation : 0.0118\n",
      "\n",
      "Annual Return : 1.5%\n",
      "Sharpe Ratio : 0.0074\n",
      "Volatility : 19%\n",
      "Maximum Drawdown : -53%\n",
      "Average Drawdown : -16%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_ukx) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Performance Metrics of Dow Jones Industrial Average (INDU) - **^DJI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return : 0.0004\n",
      "Standard Deviation : 0.0106\n",
      "\n",
      "Annual Return : 7.9%\n",
      "Sharpe Ratio : 0.0299\n",
      "Volatility : 17%\n",
      "Maximum Drawdown : -54%\n",
      "Average Drawdown : -9%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_dji) \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Performance Metrics of Dow Jones Transportation Average (TRAN) - **^DJT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return : 0.0004\n",
      "Standard Deviation : 0.0143\n",
      "\n",
      "Annual Return : 7.7%\n",
      "Sharpe Ratio : 0.0249\n",
      "Volatility : 23%\n",
      "Maximum Drawdown : -61%\n",
      "Average Drawdown : -13%\n"
     ]
    }
   ],
   "source": [
    "calc_kpi = KPIs(df_djt)  \n",
    "calc_kpi.calculate_kpi() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Classical Time Series Dual-Momentum Trading Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy\n",
    "\n",
    "1. The momentum, i.e. the percentage price change of a security, is calculated over a historical time horizon of twelve months, skipping the most recent month \n",
    "2. If momentum > threshold (here,5%=0.05) => Invest \n",
    "3. If momentum < threshold => the portfolio is moved to cash in the long-only strategy, or moved to a short position in the long-short strategy \n",
    "4. This investment decision is revisited at regular intervals of one month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before slicing : (22844, 6)\n",
      "Shape after slicing : (22823, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1927-12-30</th>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-03</th>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928-01-04</th>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close  Volume\n",
       "Date                                                                     \n",
       "1927-12-30  17.660000  17.660000  17.660000  17.660000  17.660000       0\n",
       "1928-01-03  17.760000  17.760000  17.760000  17.760000  17.760000       0\n",
       "1928-01-04  17.719999  17.719999  17.719999  17.719999  17.719999       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Shape before slicing : {df_21.shape}')\n",
    "n = len(df_21)\n",
    "# Slice the DataFrame to exclude the last 21 rows for skipping most recent month \n",
    "df_21 = df_21.iloc[:n-21]\n",
    "print(f'Shape after slicing : {df_21.shape}') \n",
    "df_21.head(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating momentum, percentage change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Momentum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-01-03</th>\n",
       "      <td>24.860001</td>\n",
       "      <td>40.770107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-04</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>39.921172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-07</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>36.851021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close   Momentum\n",
       "Date                            \n",
       "1929-01-03  24.860001  40.770107\n",
       "1929-01-04  24.850000  39.921172\n",
       "1929-01-07  24.250000  36.851021"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_days_per_month = 21\n",
    "no_of_months = 12  \n",
    "time_horizon = trading_days_per_month*no_of_months \n",
    "\n",
    "df_21['Momentum'] = df_21['Adj Close'].pct_change(periods=time_horizon)*100\n",
    "df_21.dropna(inplace=True)\n",
    "df_21.drop(columns=['Open','High','Low','Close','Volume'],axis=1,inplace=True)\n",
    "\n",
    "df_21.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of invest signals :  Signals\n",
      "1    13404\n",
      "0     9167\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>Signals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1929-01-03</th>\n",
       "      <td>24.860001</td>\n",
       "      <td>40.770107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-04</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>39.921172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929-01-07</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>36.851021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close   Momentum  Signals\n",
       "Date                                     \n",
       "1929-01-03  24.860001  40.770107        1\n",
       "1929-01-04  24.850000  39.921172        1\n",
       "1929-01-07  24.250000  36.851021        1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 5 \n",
    "df_21['Signals'] = (df_21['Momentum']>=threshold).astype(int) \n",
    "print('No of invest signals : ',df_21['Signals'].value_counts()) \n",
    "print() \n",
    "df_21.head(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Defining Function to create polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(data,degree): \n",
    "\n",
    "    feature_names = data.columns \n",
    "    # feature_names = ['Adj Close', 'momntm_30', 'momntm_60', 'momntm_90', 'momntm_120',\n",
    "    #                  'momntm_180', 'momntm_270', 'momntm_300', 'momntm_360', 'drwdwn_15',\n",
    "    #                  'drwdwn_60', 'drwdwn_90', 'drwdwn_120'] \n",
    "    \n",
    "    if data.shape[1] != len(feature_names):\n",
    "        raise ValueError(\"The number of features in the data does not match the length of feature names.\")\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    poly_feat = poly.fit_transform(data) \n",
    "    \n",
    "    feature_names_poly = poly.get_feature_names_out(input_features=feature_names)\n",
    "    \n",
    "    df_poly = pd.DataFrame(poly_feat, columns=feature_names_poly, index=data.index) \n",
    "    print(f'Shape of df_poly of degree 1 : ',data.shape) \n",
    "    print(f'Shape of df_poly of degree {degree} : ',df_poly.shape) \n",
    "    print('Number of duplicate columns : ',len(df_poly.columns)-len(set(df_poly.columns))) \n",
    "    return df_poly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_poly of degree 1 :  (22483, 17)\n",
      "Shape of df_poly of degree 2 :  (22483, 170)\n",
      "Number of duplicate columns :  0\n"
     ]
    }
   ],
   "source": [
    "x_quad = degree(data_feat,2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_poly of degree 1 :  (22483, 17)\n",
      "Shape of df_poly of degree 3 :  (22483, 1139)\n",
      "Number of duplicate columns :  0\n"
     ]
    }
   ],
   "source": [
    "x_cubic = degree(data_feat,3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating Datasets for training with Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Linear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of linear dataset before concatenation :  (22483, 17)\n",
      "Shape of linear dataset after concatenation :  (22462, 18)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of linear dataset before concatenation : ',data_feat.shape)\n",
    "x_linear = pd.concat([data_feat, df_21[['Signals']]], axis=1)\n",
    "x_linear.dropna(inplace=True) \n",
    "print('Shape of linear dataset after concatenation : ',x_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Quadratic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of quadratic dataset before concatenation :  (22483, 170)\n",
      "Shape of quadratic dataset after concatenation :  (22462, 171)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of quadratic dataset before concatenation : ',x_quad.shape)\n",
    "x_quad = pd.concat([x_quad, df_21[['Signals']]], axis=1)\n",
    "x_quad.dropna(inplace=True) \n",
    "print('Shape of quadratic dataset after concatenation : ',x_quad.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Cubic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cubic dataset before concatenation :  (22483, 1139)\n",
      "Shape of cubic dataset after concatenation :  (22462, 1140)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of cubic dataset before concatenation : ',x_cubic.shape)\n",
    "x_cubic = pd.concat([x_cubic, df_21[['Signals']]], axis=1)\n",
    "x_cubic.dropna(inplace=True) \n",
    "print('Shape of cubic dataset after concatenation : ',x_cubic.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Splitting data into Training and Testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 training and testing datasets corresponding to different time period is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_train(x_data):\n",
    "    sliced_tr1 = x_data.loc['1927-12-30':'1964-06-30'] \n",
    "    sliced_tr2 = x_data.loc['1936-09-10':'1973-03-21']\n",
    "    sliced_tr3 = x_data.loc['1945-02-02':'1981-07-15']\n",
    "    sliced_tr4 = x_data.loc['1953-07-07':'1989-11-02']\n",
    "    sliced_tr5 = x_data.loc['1961-02-15':'1997-06-04']\n",
    "    sliced_tr6 = x_data.loc['1968-12-24':'2005-03-08']\n",
    "    sliced_tr7 = x_data.loc['1977-05-20':'2013-08-09'] \n",
    "\n",
    "    return sliced_tr1, sliced_tr2, sliced_tr3, sliced_tr4, sliced_tr5, sliced_tr6, sliced_tr7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_test(x_data):\n",
    "    sliced_ts1 = x_data.loc['1964-08-03':'1973-04-23'] \n",
    "    sliced_ts2 = x_data.loc['1973-04-24':'1981-08-14']\n",
    "    sliced_ts3 = x_data.loc['1981-08-17':'1989-12-05']\n",
    "    sliced_ts4 = x_data.loc['1989-12-06':'1997-07-07']\n",
    "    sliced_ts5 = x_data.loc['1997-07-08':'2005-04-08']\n",
    "    sliced_ts6 = x_data.loc['2005-04-11':'2013-09-11']\n",
    "    sliced_ts7 = x_data.loc['2013-09-12':'2018-12-12'] \n",
    "\n",
    "    return sliced_ts1, sliced_ts2, sliced_ts3, sliced_ts4, sliced_ts5, sliced_ts6, sliced_ts7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_tr1_lin, sliced_tr2_lin, sliced_tr3_lin, sliced_tr4_lin, sliced_tr5_lin, sliced_tr6_lin, \\\n",
    "    sliced_tr7_lin = split_data_train(x_linear) \n",
    "\n",
    "sliced_ts1_lin, sliced_ts2_lin, sliced_ts3_lin, sliced_ts4_lin, sliced_ts5_lin, sliced_ts6_lin, \\\n",
    "    sliced_ts7_lin = split_data_test(x_linear) \n",
    "\n",
    "sliced_tr1_quad, sliced_tr2_quad, sliced_tr3_quad, sliced_tr4_quad, sliced_tr5_quad, sliced_tr6_quad,\\\n",
    "      sliced_tr7_quad = split_data_train(x_quad)\n",
    "\n",
    "sliced_ts1_quad, sliced_ts2_quad, sliced_ts3_quad, sliced_ts4_quad, sliced_ts5_quad, sliced_ts6_quad,\\\n",
    "      sliced_ts7_quad = split_data_test(x_quad)\n",
    "\n",
    "sliced_tr1_cub, sliced_tr2_cub, sliced_tr3_cub, sliced_tr4_cub, sliced_tr5_cub, sliced_tr6_cub, \\\n",
    "    sliced_tr7_cub = split_data_train(x_cubic)\n",
    "\n",
    "sliced_ts1_cub, sliced_ts2_cub, sliced_ts3_cub, sliced_ts4_cub, sliced_ts5_cub, sliced_ts6_cub, \\\n",
    "    sliced_ts7_cub = split_data_test(x_cubic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Class for Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model metrics calculated are cost function, accuracy, confusion matrix and classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the cost function, also known as the loss function, for logistic regression, we need to use the logistic loss function, which is commonly referred to as cross-entropy loss or log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula:\n",
    "The logistic regression cost function $J(\\theta)$ is defined as <br>\n",
    "$$J(\\theta)=\\dfrac{-1}{m}\\Sigma_{i=1}^{m} [y_i log(h_\\theta (x_i))+(1-y_i)log(1-h_\\theta (x_i))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "* $m$ = number of training examples\n",
    "* $y_i$ = true label for the $i^{th}$ example \n",
    "* $h_\\theta (x_i)$ = predicted probability of $i^{th}$ example calculated using the sigmoid function <br>\n",
    "$$h_\\theta (x_i) = \\dfrac{1}{1+e^{-\\theta^Tx_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    def __init__(self):\n",
    "        self.test_size = 0.4\n",
    "        self.random_state = 42\n",
    "\n",
    "    def scaling_x(self,X):\n",
    "        scaler = StandardScaler()\n",
    "        scaled_X = scaler.fit_transform(X)\n",
    "        return scaled_X\n",
    "    \n",
    "    def cost_func(self,model,x_test,y_test): \n",
    "        probabilities = model.predict_proba(x_test)[:,1] # Getting probabilities for class 1 (positive class)\n",
    "        # m = len(y_test) \n",
    "        epsilon = 1e-15\n",
    "        probabilities = np.clip(probabilities,epsilon,1-epsilon)\n",
    "        cost = -np.mean(y_test*np.log(probabilities)+(1-y_test)*np.log(1-probabilities))\n",
    "        return cost \n",
    "\n",
    "    def model_metrics(self,model,x_test,y_test):\n",
    "        y_pred = model.predict(x_test) \n",
    "        cost_fn = self.cost_func(model,x_test,y_test)\n",
    "        accuracy = accuracy_score(y_test,y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(f'Cost function : {cost_fn}') \n",
    "        print(f'Accuracy : {accuracy}')\n",
    "        print('Confusion Matrix : ')\n",
    "        print(conf_matrix) \n",
    "        print('Classification Report : ')\n",
    "        print(class_report) \n",
    "        return y_pred \n",
    "\n",
    "    def training_model(self,X,Y):\n",
    "        scaled_X = self.scaling_x(X)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(scaled_X,Y,test_size=self.test_size,\n",
    "                                                            shuffle=False, random_state=self.random_state)\n",
    "        model = LogisticRegression(C=1.0)   # C is the regularization parameter\n",
    "        model.fit(x_train,y_train)\n",
    "        self.model_metrics(model,x_test,y_test)\n",
    "\n",
    "logistic = logistic_regression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation of Linear, Quadratic and Cubic Combination of features on Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Evaluation on Linear Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.17383585921793046\n",
      "Accuracy : 0.9427513528909143\n",
      "Confusion Matrix : \n",
      "[[1059  190]\n",
      " [  11 2251]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.85      0.91      1249\n",
      "         1.0       0.92      1.00      0.96      2262\n",
      "\n",
      "    accuracy                           0.94      3511\n",
      "   macro avg       0.96      0.92      0.94      3511\n",
      "weighted avg       0.95      0.94      0.94      3511\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.20830680085567868\n",
      "Accuracy : 0.9439277899343544\n",
      "Confusion Matrix : \n",
      "[[1340   54]\n",
      " [ 151 2111]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.96      0.93      1394\n",
      "         1.0       0.98      0.93      0.95      2262\n",
      "\n",
      "    accuracy                           0.94      3656\n",
      "   macro avg       0.94      0.95      0.94      3656\n",
      "weighted avg       0.95      0.94      0.94      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.39655830681254806\n",
      "Accuracy : 0.9130196936542669\n",
      "Confusion Matrix : \n",
      "[[1588  159]\n",
      " [ 159 1750]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      1747\n",
      "         1.0       0.92      0.92      0.92      1909\n",
      "\n",
      "    accuracy                           0.91      3656\n",
      "   macro avg       0.91      0.91      0.91      3656\n",
      "weighted avg       0.91      0.91      0.91      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.37925108776770156\n",
      "Accuracy : 0.9291575492341356\n",
      "Confusion Matrix : \n",
      "[[1268   63]\n",
      " [ 196 2129]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.95      0.91      1331\n",
      "         1.0       0.97      0.92      0.94      2325\n",
      "\n",
      "    accuracy                           0.93      3656\n",
      "   macro avg       0.92      0.93      0.92      3656\n",
      "weighted avg       0.93      0.93      0.93      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.3895809950524085\n",
      "Accuracy : 0.9439277899343544\n",
      "Confusion Matrix : \n",
      "[[ 755  168]\n",
      " [  37 2696]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.82      0.88       923\n",
      "         1.0       0.94      0.99      0.96      2733\n",
      "\n",
      "    accuracy                           0.94      3656\n",
      "   macro avg       0.95      0.90      0.92      3656\n",
      "weighted avg       0.94      0.94      0.94      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.44861443325774997\n",
      "Accuracy : 0.9318927789934355\n",
      "Confusion Matrix : \n",
      "[[ 897  239]\n",
      " [  10 2510]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88      1136\n",
      "         1.0       0.91      1.00      0.95      2520\n",
      "\n",
      "    accuracy                           0.93      3656\n",
      "   macro avg       0.95      0.89      0.92      3656\n",
      "weighted avg       0.94      0.93      0.93      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.35004185284400624\n",
      "Accuracy : 0.9165754923413567\n",
      "Confusion Matrix : \n",
      "[[1322  141]\n",
      " [ 164 2029]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.90      1463\n",
      "         1.0       0.94      0.93      0.93      2193\n",
      "\n",
      "    accuracy                           0.92      3656\n",
      "   macro avg       0.91      0.91      0.91      3656\n",
      "weighted avg       0.92      0.92      0.92      3656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_lin = [sliced_tr1_lin, sliced_tr2_lin, sliced_tr3_lin, sliced_tr4_lin, sliced_tr5_lin,\n",
    "                 sliced_tr6_lin, sliced_tr7_lin] \n",
    "\n",
    "for df in sliced_df_lin:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_lin = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Evaluation on Quadratic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.7567384382599395\n",
      "Accuracy : 0.8968954713756765\n",
      "Confusion Matrix : \n",
      "[[ 999  250]\n",
      " [ 112 2150]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.80      0.85      1249\n",
      "         1.0       0.90      0.95      0.92      2262\n",
      "\n",
      "    accuracy                           0.90      3511\n",
      "   macro avg       0.90      0.88      0.88      3511\n",
      "weighted avg       0.90      0.90      0.90      3511\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.0184041307017997\n",
      "Accuracy : 0.8569474835886215\n",
      "Confusion Matrix : \n",
      "[[1346   48]\n",
      " [ 475 1787]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.97      0.84      1394\n",
      "         1.0       0.97      0.79      0.87      2262\n",
      "\n",
      "    accuracy                           0.86      3656\n",
      "   macro avg       0.86      0.88      0.85      3656\n",
      "weighted avg       0.88      0.86      0.86      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.5288016906897562\n",
      "Accuracy : 0.9105579868708972\n",
      "Confusion Matrix : \n",
      "[[1519  228]\n",
      " [  99 1810]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.87      0.90      1747\n",
      "         1.0       0.89      0.95      0.92      1909\n",
      "\n",
      "    accuracy                           0.91      3656\n",
      "   macro avg       0.91      0.91      0.91      3656\n",
      "weighted avg       0.91      0.91      0.91      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.9209438136236608\n",
      "Accuracy : 0.9245076586433261\n",
      "Confusion Matrix : \n",
      "[[1175  156]\n",
      " [ 120 2205]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.88      0.89      1331\n",
      "         1.0       0.93      0.95      0.94      2325\n",
      "\n",
      "    accuracy                           0.92      3656\n",
      "   macro avg       0.92      0.92      0.92      3656\n",
      "weighted avg       0.92      0.92      0.92      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.47828467062922453\n",
      "Accuracy : 0.9387308533916849\n",
      "Confusion Matrix : \n",
      "[[ 736  187]\n",
      " [  37 2696]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.80      0.87       923\n",
      "         1.0       0.94      0.99      0.96      2733\n",
      "\n",
      "    accuracy                           0.94      3656\n",
      "   macro avg       0.94      0.89      0.91      3656\n",
      "weighted avg       0.94      0.94      0.94      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.798457978728192\n",
      "Accuracy : 0.9297045951859956\n",
      "Confusion Matrix : \n",
      "[[ 985  151]\n",
      " [ 106 2414]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.87      0.88      1136\n",
      "         1.0       0.94      0.96      0.95      2520\n",
      "\n",
      "    accuracy                           0.93      3656\n",
      "   macro avg       0.92      0.91      0.92      3656\n",
      "weighted avg       0.93      0.93      0.93      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.055153912199586\n",
      "Accuracy : 0.8966083150984683\n",
      "Confusion Matrix : \n",
      "[[1180  283]\n",
      " [  95 2098]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.81      0.86      1463\n",
      "         1.0       0.88      0.96      0.92      2193\n",
      "\n",
      "    accuracy                           0.90      3656\n",
      "   macro avg       0.90      0.88      0.89      3656\n",
      "weighted avg       0.90      0.90      0.90      3656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_quad = [sliced_tr1_quad, sliced_tr2_quad, sliced_tr3_quad, sliced_tr4_quad, sliced_tr5_quad,\n",
    "                 sliced_tr6_quad, sliced_tr7_quad] \n",
    "\n",
    "for df in sliced_df_quad:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_quad = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Evaluation on Cubic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.47779480688378434\n",
      "Accuracy : 0.8857875249216748\n",
      "Confusion Matrix : \n",
      "[[1133  116]\n",
      " [ 285 1977]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.91      0.85      1249\n",
      "         1.0       0.94      0.87      0.91      2262\n",
      "\n",
      "    accuracy                           0.89      3511\n",
      "   macro avg       0.87      0.89      0.88      3511\n",
      "weighted avg       0.89      0.89      0.89      3511\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.6688470402539024\n",
      "Accuracy : 0.9100109409190372\n",
      "Confusion Matrix : \n",
      "[[1345   49]\n",
      " [ 280 1982]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.96      0.89      1394\n",
      "         1.0       0.98      0.88      0.92      2262\n",
      "\n",
      "    accuracy                           0.91      3656\n",
      "   macro avg       0.90      0.92      0.91      3656\n",
      "weighted avg       0.92      0.91      0.91      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.8169607640623379\n",
      "Accuracy : 0.9053610503282276\n",
      "Confusion Matrix : \n",
      "[[1520  227]\n",
      " [ 119 1790]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.87      0.90      1747\n",
      "         1.0       0.89      0.94      0.91      1909\n",
      "\n",
      "    accuracy                           0.91      3656\n",
      "   macro avg       0.91      0.90      0.90      3656\n",
      "weighted avg       0.91      0.91      0.91      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.2853255032124007\n",
      "Accuracy : 0.9105579868708972\n",
      "Confusion Matrix : \n",
      "[[1112  219]\n",
      " [ 108 2217]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.84      0.87      1331\n",
      "         1.0       0.91      0.95      0.93      2325\n",
      "\n",
      "    accuracy                           0.91      3656\n",
      "   macro avg       0.91      0.89      0.90      3656\n",
      "weighted avg       0.91      0.91      0.91      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.2330988767015765\n",
      "Accuracy : 0.8897702407002188\n",
      "Confusion Matrix : \n",
      "[[ 783  140]\n",
      " [ 263 2470]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.85      0.80       923\n",
      "         1.0       0.95      0.90      0.92      2733\n",
      "\n",
      "    accuracy                           0.89      3656\n",
      "   macro avg       0.85      0.88      0.86      3656\n",
      "weighted avg       0.90      0.89      0.89      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 2.2440721684134597\n",
      "Accuracy : 0.8684354485776805\n",
      "Confusion Matrix : \n",
      "[[ 867  269]\n",
      " [ 212 2308]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.76      0.78      1136\n",
      "         1.0       0.90      0.92      0.91      2520\n",
      "\n",
      "    accuracy                           0.87      3656\n",
      "   macro avg       0.85      0.84      0.84      3656\n",
      "weighted avg       0.87      0.87      0.87      3656\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.8459073017847323\n",
      "Accuracy : 0.8810175054704595\n",
      "Confusion Matrix : \n",
      "[[1137  326]\n",
      " [ 109 2084]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.78      0.84      1463\n",
      "         1.0       0.86      0.95      0.91      2193\n",
      "\n",
      "    accuracy                           0.88      3656\n",
      "   macro avg       0.89      0.86      0.87      3656\n",
      "weighted avg       0.88      0.88      0.88      3656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_cub = [sliced_tr1_cub, sliced_tr2_cub, sliced_tr3_cub, sliced_tr4_cub, sliced_tr5_cub,\n",
    "                 sliced_tr6_cub, sliced_tr7_cub] \n",
    "\n",
    "for df in sliced_df_cub:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_cub = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluation of Linear, Quadratic and Cubic Combination of features on Testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Evaluation on Linear Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.17219668948732228\n",
      "Accuracy : 0.9286536248561565\n",
      "Confusion Matrix : \n",
      "[[312  33]\n",
      " [ 29 495]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.90      0.91       345\n",
      "         1.0       0.94      0.94      0.94       524\n",
      "\n",
      "    accuracy                           0.93       869\n",
      "   macro avg       0.93      0.92      0.93       869\n",
      "weighted avg       0.93      0.93      0.93       869\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.5563159604138853\n",
      "Accuracy : 0.7312722948870393\n",
      "Confusion Matrix : \n",
      "[[224  17]\n",
      " [209 391]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.93      0.66       241\n",
      "         1.0       0.96      0.65      0.78       600\n",
      "\n",
      "    accuracy                           0.73       841\n",
      "   macro avg       0.74      0.79      0.72       841\n",
      "weighted avg       0.83      0.73      0.74       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.256146633396117\n",
      "Accuracy : 0.9476813317479191\n",
      "Confusion Matrix : \n",
      "[[250   1]\n",
      " [ 43 547]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      1.00      0.92       251\n",
      "         1.0       1.00      0.93      0.96       590\n",
      "\n",
      "    accuracy                           0.95       841\n",
      "   macro avg       0.93      0.96      0.94       841\n",
      "weighted avg       0.95      0.95      0.95       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.25732029997376377\n",
      "Accuracy : 0.9217731421121251\n",
      "Confusion Matrix : \n",
      "[[151  27]\n",
      " [ 33 556]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       178\n",
      "         1.0       0.95      0.94      0.95       589\n",
      "\n",
      "    accuracy                           0.92       767\n",
      "   macro avg       0.89      0.90      0.89       767\n",
      "weighted avg       0.92      0.92      0.92       767\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.133415159607294\n",
      "Accuracy : 0.6030729833546735\n",
      "Confusion Matrix : \n",
      "[[365   6]\n",
      " [304 106]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.98      0.70       371\n",
      "         1.0       0.95      0.26      0.41       410\n",
      "\n",
      "    accuracy                           0.60       781\n",
      "   macro avg       0.75      0.62      0.55       781\n",
      "weighted avg       0.76      0.60      0.55       781\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.4566763753860998\n",
      "Accuracy : 0.8386336866902238\n",
      "Confusion Matrix : \n",
      "[[110  83]\n",
      " [ 54 602]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.57      0.62       193\n",
      "         1.0       0.88      0.92      0.90       656\n",
      "\n",
      "    accuracy                           0.84       849\n",
      "   macro avg       0.77      0.74      0.76       849\n",
      "weighted avg       0.83      0.84      0.83       849\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.22788844216277476\n",
      "Accuracy : 0.9558541266794626\n",
      "Confusion Matrix : \n",
      "[[  0  23]\n",
      " [  0 498]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        23\n",
      "         1.0       0.96      1.00      0.98       498\n",
      "\n",
      "    accuracy                           0.96       521\n",
      "   macro avg       0.48      0.50      0.49       521\n",
      "weighted avg       0.91      0.96      0.93       521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_lin = [sliced_ts1_lin, sliced_ts2_lin, sliced_ts3_lin, sliced_ts4_lin, sliced_ts5_lin,\n",
    "                 sliced_ts6_lin, sliced_ts7_lin] \n",
    "\n",
    "for df in sliced_df_lin:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_linear = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Evaluation on Quadratic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.3570732121684744\n",
      "Accuracy : 0.9125431530494822\n",
      "Confusion Matrix : \n",
      "[[312  33]\n",
      " [ 43 481]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       345\n",
      "         1.0       0.94      0.92      0.93       524\n",
      "\n",
      "    accuracy                           0.91       869\n",
      "   macro avg       0.91      0.91      0.91       869\n",
      "weighted avg       0.91      0.91      0.91       869\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.5220162797269994\n",
      "Accuracy : 0.7859690844233056\n",
      "Confusion Matrix : \n",
      "[[154  87]\n",
      " [ 93 507]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.64      0.63       241\n",
      "         1.0       0.85      0.84      0.85       600\n",
      "\n",
      "    accuracy                           0.79       841\n",
      "   macro avg       0.74      0.74      0.74       841\n",
      "weighted avg       0.79      0.79      0.79       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.8330508548975208\n",
      "Accuracy : 0.9239001189060642\n",
      "Confusion Matrix : \n",
      "[[232  19]\n",
      " [ 45 545]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.92      0.88       251\n",
      "         1.0       0.97      0.92      0.94       590\n",
      "\n",
      "    accuracy                           0.92       841\n",
      "   macro avg       0.90      0.92      0.91       841\n",
      "weighted avg       0.93      0.92      0.92       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.1888055742042361\n",
      "Accuracy : 0.9295958279009127\n",
      "Confusion Matrix : \n",
      "[[142  36]\n",
      " [ 18 571]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.80      0.84       178\n",
      "         1.0       0.94      0.97      0.95       589\n",
      "\n",
      "    accuracy                           0.93       767\n",
      "   macro avg       0.91      0.88      0.90       767\n",
      "weighted avg       0.93      0.93      0.93       767\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.3019800123310425\n",
      "Accuracy : 0.6798975672215108\n",
      "Confusion Matrix : \n",
      "[[358  13]\n",
      " [237 173]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.96      0.74       371\n",
      "         1.0       0.93      0.42      0.58       410\n",
      "\n",
      "    accuracy                           0.68       781\n",
      "   macro avg       0.77      0.69      0.66       781\n",
      "weighted avg       0.77      0.68      0.66       781\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.3430576428140795\n",
      "Accuracy : 0.8751472320376914\n",
      "Confusion Matrix : \n",
      "[[121  72]\n",
      " [ 34 622]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.63      0.70       193\n",
      "         1.0       0.90      0.95      0.92       656\n",
      "\n",
      "    accuracy                           0.88       849\n",
      "   macro avg       0.84      0.79      0.81       849\n",
      "weighted avg       0.87      0.88      0.87       849\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.22547601612184331\n",
      "Accuracy : 0.9616122840690979\n",
      "Confusion Matrix : \n",
      "[[  3  20]\n",
      " [  0 498]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.13      0.23        23\n",
      "         1.0       0.96      1.00      0.98       498\n",
      "\n",
      "    accuracy                           0.96       521\n",
      "   macro avg       0.98      0.57      0.61       521\n",
      "weighted avg       0.96      0.96      0.95       521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_quad = [sliced_ts1_quad, sliced_ts2_quad, sliced_ts3_quad, sliced_ts4_quad, sliced_ts5_quad,\n",
    "                 sliced_ts6_quad, sliced_ts7_quad] \n",
    "\n",
    "for df in sliced_df_quad:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_quadratic = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Evaluation on Cubic Combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cost function : 0.6590054013156437\n",
      "Accuracy : 0.9021864211737629\n",
      "Confusion Matrix : \n",
      "[[311  34]\n",
      " [ 51 473]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.90      0.88       345\n",
      "         1.0       0.93      0.90      0.92       524\n",
      "\n",
      "    accuracy                           0.90       869\n",
      "   macro avg       0.90      0.90      0.90       869\n",
      "weighted avg       0.90      0.90      0.90       869\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.6961237144636168\n",
      "Accuracy : 0.7740784780023782\n",
      "Confusion Matrix : \n",
      "[[135 106]\n",
      " [ 84 516]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.56      0.59       241\n",
      "         1.0       0.83      0.86      0.84       600\n",
      "\n",
      "    accuracy                           0.77       841\n",
      "   macro avg       0.72      0.71      0.72       841\n",
      "weighted avg       0.77      0.77      0.77       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 1.5324425049618304\n",
      "Accuracy : 0.9441141498216409\n",
      "Confusion Matrix : \n",
      "[[251   0]\n",
      " [ 47 543]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      1.00      0.91       251\n",
      "         1.0       1.00      0.92      0.96       590\n",
      "\n",
      "    accuracy                           0.94       841\n",
      "   macro avg       0.92      0.96      0.94       841\n",
      "weighted avg       0.95      0.94      0.95       841\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.5184973765734923\n",
      "Accuracy : 0.894393741851369\n",
      "Confusion Matrix : \n",
      "[[133  45]\n",
      " [ 36 553]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.75      0.77       178\n",
      "         1.0       0.92      0.94      0.93       589\n",
      "\n",
      "    accuracy                           0.89       767\n",
      "   macro avg       0.86      0.84      0.85       767\n",
      "weighted avg       0.89      0.89      0.89       767\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.9013874699004526\n",
      "Accuracy : 0.7490396927016645\n",
      "Confusion Matrix : \n",
      "[[345  26]\n",
      " [170 240]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.93      0.78       371\n",
      "         1.0       0.90      0.59      0.71       410\n",
      "\n",
      "    accuracy                           0.75       781\n",
      "   macro avg       0.79      0.76      0.74       781\n",
      "weighted avg       0.79      0.75      0.74       781\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.45090495785112783\n",
      "Accuracy : 0.8539458186101295\n",
      "Confusion Matrix : \n",
      "[[130  63]\n",
      " [ 61 595]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.67      0.68       193\n",
      "         1.0       0.90      0.91      0.91       656\n",
      "\n",
      "    accuracy                           0.85       849\n",
      "   macro avg       0.79      0.79      0.79       849\n",
      "weighted avg       0.85      0.85      0.85       849\n",
      "\n",
      "==========================================================================================\n",
      "Cost function : 0.29079844392810933\n",
      "Accuracy : 0.9596928982725528\n",
      "Confusion Matrix : \n",
      "[[  2  21]\n",
      " [  0 498]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.09      0.16        23\n",
      "         1.0       0.96      1.00      0.98       498\n",
      "\n",
      "    accuracy                           0.96       521\n",
      "   macro avg       0.98      0.54      0.57       521\n",
      "weighted avg       0.96      0.96      0.94       521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sliced_df_cub = [sliced_ts1_cub, sliced_ts2_cub, sliced_ts3_cub, sliced_ts4_cub, sliced_ts5_cub,\n",
    "                 sliced_ts6_cub, sliced_ts7_cub] \n",
    "\n",
    "for df in sliced_df_cub:\n",
    "    print('='*90)  \n",
    "    X = df.drop(columns=['Signals'],axis=1)\n",
    "    Y = df['Signals'] \n",
    "    y_pred_cubic = logistic.training_model(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **On average, the accuracy score of the cubic model is higher than that of the quadratic and linear models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calculating Key Performance Indicators of various Logistic regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1 Benchmark SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2 Logistic Regression Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3 Logistic Regression Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.4 Logistic Regression Cubic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
